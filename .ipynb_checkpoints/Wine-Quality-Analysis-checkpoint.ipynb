{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUoAmnerXPuM"
   },
   "source": [
    "# MIT License\n",
    "\n",
    "Copyright (c) 2022 Karan Raman Agrawal\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gx8KmA45XYZu"
   },
   "source": [
    "# Abstract:\n",
    "This dataset about wine and its quality based on some features like alcohol content, suplphate, sugar, etc. It is used in wine industry to check the quality of wine.\n",
    "\n",
    "# Aim: \n",
    "In this assignment we aim to create a complete ML project with \n",
    "- Checking Data Quality\n",
    "- Feature Selection\n",
    "- Modeling - Training Models, Selecting Best Model, Hyperparameter Tuning\n",
    "- Model Interpretibility\n",
    "- Reports and Visualizations\n",
    "\n",
    "\n",
    "#Variable Description\n",
    "1) **Fixed Acidity** - Quantity of non volatile acids in the wine.               \n",
    "2) **Volatile Acidity** - Quantity of volatile acids like acetic acid in the wine.                                                                     \n",
    "3) **Citric Acid** - Quantity of citric acid in wine, which gives sour taste to wine                                                             \n",
    "4) **Residual Sugar** - Amount of sugar left in the wine after the process of fermentation                                                         \n",
    "5) **Chlorides** - Quantity of salts in wine                               \n",
    "6) **Free Sulphur dioxide** - Amount of free form of SO2, which prevents the oxidation of wine                                                 \n",
    "7) **Total Sulphur dioxide** - Total Amount of SO2 - free and bonded in wine.    \n",
    "8) **Density** - Density of wine       \n",
    "9) **pH** - pH value of wine                          \n",
    "10) **Sulphates** - Amount of sulphates in wine used as wine additive           \n",
    "11) **Alcohol** - Quantity of alcohol on wine                                   \n",
    "12) **Quality** - Output variable between (0 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swANGCWdXOcS",
    "outputId": "829a77d9-f9d4-41de-c0af-aef771553d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karan\\appdata\\roaming\\python\\python310\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2022.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.8.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorama>=0.3.8 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: future in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.18.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h2o in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.36.1.4)\n",
      "Requirement already satisfied: requests in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h2o) (2.28.1)\n",
      "Requirement already satisfied: future in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h2o) (0.18.2)\n",
      "Requirement already satisfied: tabulate in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h2o) (0.8.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->h2o) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->h2o) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karan\\appdata\\roaming\\python\\python310\\site-packages (from requests->h2o) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->h2o) (2022.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "C:\\Users\\Karan\\AppData\\Local\\Temp\\ipykernel_33000\\2557284720.py:18: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  from distutils.util import strtobool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fitter in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: scipy>=0.18 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fitter) (1.9.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fitter) (1.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fitter) (1.22.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fitter) (1.4.4)\n",
      "Requirement already satisfied: easydev in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fitter) (0.12.0)\n",
      "Requirement already satisfied: click in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fitter) (8.1.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fitter) (3.5.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->fitter) (0.4.5)\n",
      "Requirement already satisfied: pexpect in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from easydev->fitter) (4.8.0)\n",
      "Requirement already satisfied: colorlog in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from easydev->fitter) (6.7.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fitter) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fitter) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fitter) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fitter) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fitter) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fitter) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fitter) (4.37.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->fitter) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->fitter) (1.16.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pexpect->easydev->fitter) (0.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fancyimpute in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: cvxpy in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fancyimpute) (1.2.1)\n",
      "Requirement already satisfied: nose in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fancyimpute) (1.3.7)\n",
      "Requirement already satisfied: pytest in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fancyimpute) (7.0.1)\n",
      "Requirement already satisfied: cvxopt in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fancyimpute) (1.3.0)\n",
      "Requirement already satisfied: knnimpute>=0.1.0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fancyimpute) (0.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fancyimpute) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.10 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from knnimpute>=0.1.0->fancyimpute) (1.22.4)\n",
      "Requirement already satisfied: six in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from knnimpute>=0.1.0->fancyimpute) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.24.2->fancyimpute) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.24.2->fancyimpute) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.24.2->fancyimpute) (1.1.0)\n",
      "Requirement already satisfied: scs>=1.1.6 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cvxpy->fancyimpute) (3.2.0)\n",
      "Requirement already satisfied: osqp>=0.4.1 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cvxpy->fancyimpute) (0.6.2.post5)\n",
      "Requirement already satisfied: ecos>=2 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cvxpy->fancyimpute) (2.0.10)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytest->fancyimpute) (1.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytest->fancyimpute) (21.3)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytest->fancyimpute) (1.4.1)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytest->fancyimpute) (2.0.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytest->fancyimpute) (22.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytest->fancyimpute) (0.4.5)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytest->fancyimpute) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytest->fancyimpute) (1.1.1)\n",
      "Requirement already satisfied: qdldl in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from osqp>=0.4.1->cvxpy->fancyimpute) (0.1.5.post2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->pytest->fancyimpute) (3.0.9)\n",
      "Requirement already satisfied: shap in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.41.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from shap) (1.9.1)\n",
      "Requirement already satisfied: tqdm>4.25.0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from shap) (4.64.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from shap) (21.3)\n",
      "Requirement already satisfied: slicer==0.0.7 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from shap) (2.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from shap) (1.4.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from shap) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from shap) (1.1.2)\n",
      "Requirement already satisfied: numba in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from shap) (0.56.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>20.9->shap) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>4.25.0->shap) (0.4.5)\n",
      "Requirement already satisfied: setuptools<60 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from numba->shap) (58.3.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from numba->shap) (0.39.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->shap) (2022.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->shap) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->shap) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\karan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "##Installing all the required libraries for the Assignment\n",
    "!pip install requests\n",
    "!pip install tabulate\n",
    "!pip install \"colorama>=0.3.8\"\n",
    "!pip install future\n",
    "!pip install h2o\n",
    "!pip install fitter\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import optparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from distutils.util import strtobool\n",
    "\n",
    "# Importing all the libararies required for the assignment\n",
    "import h2o\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import seaborn as sns\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "from h2o.automl import H2OAutoML\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (16, 8)})\n",
    "\n",
    "# Data Imputation\n",
    "!pip install fancyimpute\n",
    "from fancyimpute import IterativeImputer as MICE\n",
    "\n",
    "# Model Interpretation\n",
    "!pip install shap\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvuC40x71CfH"
   },
   "source": [
    "### What question are you trying to answer? How did you frame it as an ML task?\n",
    "\n",
    "- Here I am trying to **predict the quality of wine** given some of its features like Alcohol, Sulphate, Sugar content. The quality of wine is denoted by numbers 3 to 8 where 3 is bad quality and 8 is the best. \n",
    "- The quality of wine is represented by a integer value from 3 to 8. This is a classical classification problem. But if we look at it as a purely classification problem, it may not be a fair play. E.g. Let's say that the predicted quality of wine is 7, but the actual quality was 8. In this case we came pretty close in predicting it's quality but as a purely classification problem, it is still a wrong prediction. Hence, another way is to approch this problem as a regression problem instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utZK6GPq1m4M"
   },
   "source": [
    "### What is human-level performance on that task? What level of performance is needed?\n",
    "- Sensory tests are performed by human taste experts which is a very complex and expensive process. This process is also nor very accurate as the wine testers are testing alot of wines which introduces a human bias. In this method we have taken 11 features of wine which can easily and quickly predict the quality of wine without little to none human interference, saving both money and time and giving a very accurate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NTEx2RAN3CWT"
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Reading Data into Pandas Dataframe\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m wine_df_pandas \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://github.com/KaranAgrawal1509/DataScienceMethodsAndTools/raw/main/Wine_QT.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    664\u001b[0m     dialect,\n\u001b[0;32m    665\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:667\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    664\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 667\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    676\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:336\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    335\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[0;32m    337\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:236\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# Reading Data into Pandas Dataframe\n",
    "wine_df_pandas = pd.read_csv(\n",
    "    \"https://github.com/KaranAgrawal1509/Wine-Quality-Analysis/raw/main/WineQT.xlsx\"\n",
    ")  # Reading CSV file into Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lw2OWg2s3Gym"
   },
   "outputs": [],
   "source": [
    "wine_df_pandas.head()\n",
    "wine_df_pandas = wine_df_pandas.drop([\"Id\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKPvuu4M2NUU"
   },
   "source": [
    "### Where did the feature data and label come from?\n",
    "\n",
    "- I used the Wine Quality Dataset from the UCI machine learning repository. The dataset contains 1143 different wine examples levels of the eleven physicochemical properties were determined: \n",
    "    - fixed acidity (g(tartaric acid)/L), \n",
    "    - volatile acidity (g(acetic acid)/L), \n",
    "    - citric acid (g/L), \n",
    "    - residual sugar (g/L), \n",
    "    - chlorides(g(sodium chloride)/L), \n",
    "    - free sulfur dioxide (mg/L), \n",
    "    - total sulfur dioxide (mg/L), \n",
    "    - density (g/ml), \n",
    "    - pH , \n",
    "    - sulphates (g(potassium sulphate)/L), \n",
    "    - alcohol ( % vol.).\n",
    "    - The quality of the wines is a score between 3 and 8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAHXWajq4bYq"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSeFYVDD3IjS",
    "outputId": "261d1ec4-5f60-4e7d-d24a-e3458aabc881"
   },
   "outputs": [],
   "source": [
    "# Checking Data Type of each variable\n",
    "wine_df_pandas.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4vmjmg03LT1",
    "outputId": "0a451837-2cee-45f2-fab1-7ef851f94b55"
   },
   "outputs": [],
   "source": [
    "# Checking for any null values in the dataset, as we hav no null values - we are good to go.\n",
    "wine_df_pandas.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6xot3yw3Mrf",
    "outputId": "c6872c0e-ecc3-42a2-ebfd-2043979f66fe"
   },
   "outputs": [],
   "source": [
    "# Shape of the dataframe - Here, we have 1143 samples of data with 12 predictors and 1 result\n",
    "wine_df_pandas.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mpVkssE3OBl",
    "outputId": "fd651150-7d4b-47a0-f3c1-911253369688"
   },
   "outputs": [],
   "source": [
    "columns = list(wine_df_pandas.columns)\n",
    "print(columns)  # Name of all predictors and result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UJj3tnDV3QVx",
    "outputId": "b869b2d6-bfc9-413a-980f-654edbdc1769"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "for i in wine_df_pandas.columns:\n",
    "    values = wine_df_pandas[i].values\n",
    "    f = Fitter(values, distributions=get_common_distributions())\n",
    "    f.fit()\n",
    "    print(f.summary())\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.xlabel(i)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 713
    },
    "id": "3P3IXs8m3SVy",
    "outputId": "c578d3cb-b236-40fb-aa86-478b221a3a74"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "sns.heatmap(\n",
    "    wine_df_pandas.corr(), annot=True, cmap=\"Blues\"\n",
    ")  # Heatmap showing correlation between the attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3pLsJvfh3V-p",
    "outputId": "206b8417-046f-4bc4-e2b9-cf111d763345"
   },
   "outputs": [],
   "source": [
    "# All attributes are plotted against each other to see the pattern of distribution\n",
    "sns.pairplot(wine_df_pandas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "id": "3yHvttoLy1yH",
    "outputId": "f8f12d00-77a1-40d2-9cf8-175d6ac13fdb"
   },
   "outputs": [],
   "source": [
    "names = wine_df_pandas[\n",
    "    [\n",
    "        \"volatile_acidity\",\n",
    "        \"chlorides\",\n",
    "        \"total_sulfur_dioxide\",\n",
    "        \"sulphates\",\n",
    "        \"alcohol\",\n",
    "        \"pH\",\n",
    "        \"quality\",\n",
    "    ]\n",
    "].columns.drop(\"quality\")\n",
    "ncols = len(names)\n",
    "fig, axes = plt.subplots(1, ncols)\n",
    "for name, ax in zip(names, axes.flatten()):\n",
    "    sns.boxplot(\n",
    "        y=name,\n",
    "        x=\"quality\",\n",
    "        data=wine_df_pandas[\n",
    "            [\n",
    "                \"volatile_acidity\",\n",
    "                \"chlorides\",\n",
    "                \"total_sulfur_dioxide\",\n",
    "                \"sulphates\",\n",
    "                \"alcohol\",\n",
    "                \"pH\",\n",
    "                \"quality\",\n",
    "            ]\n",
    "        ],\n",
    "        orient=\"v\",\n",
    "        ax=ax,\n",
    "    )\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoMy9eQWzJhp"
   },
   "source": [
    "### What kind of data exploration did you do?\n",
    "**Preliminary findings from the EDA.**\n",
    "- **Datatype Check** - All the features in the dataset are Integers/ Float.\n",
    "- **Missing Data Check** - Our Dataset Didn't had any missing values in any of the features.\n",
    "- **Distribution of training data** - I checked the probability distribution of each and every feature in the training dataset\n",
    "- **Correlation Check** - I performed a correlation check on the dataset using heatmap and pirplot, the findings from it was that the didn't had any significant multi colinearity issues. \n",
    "- **Barplot** - Performed Barplot analysis to findout how a variable impacts the quality of wine\n",
    "\n",
    "The above graphs are a cohesive representation of how quality of wine is changing w.r.t. all the significant variables.\n",
    "- A few findings which we can interpret from the above graph as\n",
    "    - Quality of wine increase as the volatile_acidity decrease\n",
    "    - Quality of wine increase as the level of chlorides decrease\n",
    "    - Quality of wine increase as the level of sulphates increase\n",
    "    - Quality of wine increase as the level of alcohol increase\n",
    "\n",
    "- These are the preliminary findings about how the individual parameters are affecting the quality of wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ixf8FB744u6u"
   },
   "source": [
    "### Data Cleaning\n",
    "- This dataset was relatively cleaner with no missing values. But handing missing values is one of the most important tasks in any Data Science project. \n",
    "\n",
    "### Handling Missing Data\n",
    "- In any ML modeling task, one of the very important task is to check if we have any missing values in our dataset. If there are missing values in the dataset it can horribly change the results and make our model inefficient.\n",
    "- But there are ways to tackle this problem, we will discuss these methods in a bit detail below\n",
    "\n",
    "### How to handle missing data - Data Imputation techniques\n",
    "1.   Delete rows with missing data\n",
    "2.   Fill the missing data points by either mean/mode/median\n",
    "3.   Model a regression with rest of the feature as input and the missing value as the target i.e. MICE Imputation\n",
    "\n",
    "Our dataset doesn't have any missing values, lets deliberately create some missing values and have a comparative analysis of how effectively each method is recovering the data back.\n",
    "\n",
    "We will remove 1%, 5% and 10% of the data from alcohol in our dataset and see how well Mean Imputation and MICE imputation will recover the data back\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqlYhfYM3zWT"
   },
   "outputs": [],
   "source": [
    "df_imputation = wine_df_pandas.copy()\n",
    "# Creating three extra coloumns of Alcohol for performing Data Imputation techniques\n",
    "df_imputation[\"alcohol_1_percent\"] = df_imputation[[\"alcohol\"]]\n",
    "df_imputation[\"alcohol_5_percent\"] = df_imputation[[\"alcohol\"]]\n",
    "df_imputation[\"alcohol_10_percent\"] = df_imputation[[\"alcohol\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_cu2xj84sXb"
   },
   "outputs": [],
   "source": [
    "# Function to calculate percentage of missing data in dataset\n",
    "def get_percent_missing(dataframe):\n",
    "\n",
    "    percent_missing = dataframe.isnull().sum() * 100 / len(dataframe)\n",
    "    missing_value_df = pd.DataFrame(\n",
    "        {\"column_name\": dataframe.columns, \"percent_missing\": percent_missing}\n",
    "    )\n",
    "    return missing_value_df\n",
    "\n",
    "\n",
    "# Function to create missing values\n",
    "def create_missing(dataframe, percent, col):\n",
    "    dataframe.loc[dataframe.sample(frac=percent).index, col] = np.nan\n",
    "\n",
    "\n",
    "# Modified function to impute data using mean imputation\n",
    "def mean_impute(train_df):\n",
    "    mm_impute_train_df = train_df.copy()  # Make a copy of dataframe for imputation\n",
    "    mm_impute_train_df[\"alcohol_1_percent\"] = mm_impute_train_df[\n",
    "        \"alcohol_1_percent\"\n",
    "    ].fillna(mm_impute_train_df[\"alcohol\"].mean())\n",
    "    mm_impute_train_df[\"alcohol_5_percent\"] = mm_impute_train_df[\n",
    "        \"alcohol_5_percent\"\n",
    "    ].fillna(mm_impute_train_df[\"alcohol\"].mean())\n",
    "    mm_impute_train_df[\"alcohol_10_percent\"] = mm_impute_train_df[\n",
    "        \"alcohol_10_percent\"\n",
    "    ].fillna(mm_impute_train_df[\"alcohol\"].mean())\n",
    "    return mm_impute_train_df\n",
    "\n",
    "\n",
    "# Function to impute data using MICE method\n",
    "def mice_impute(train_df):\n",
    "    mice_imputed_train_df = pd.DataFrame(\n",
    "        MICE().fit_transform(train_df), columns=list(train_df.columns)\n",
    "    )\n",
    "    return mice_imputed_train_df\n",
    "\n",
    "\n",
    "# Function to calculate how well the data has been recovered after performing data imputation\n",
    "def percentage_change(l1, l2):\n",
    "    percent_change = abs(l2 - l1) / (l1 + 0.000000001)\n",
    "    avg_change = (percent_change.sum() / percent_change.count()) * 100\n",
    "    return avg_change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OtgRYjE-A0Cj",
    "outputId": "de2e52aa-7b9a-430e-f048-1a499f2650a5"
   },
   "outputs": [],
   "source": [
    "# Checking for % missing values in dataset\n",
    "print(get_percent_missing(df_imputation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4wWyIO0A2kw"
   },
   "outputs": [],
   "source": [
    "# Creating missing values in respected coloumns to perform data imputation\n",
    "create_missing(df_imputation, 0.01, \"alcohol_1_percent\")\n",
    "create_missing(df_imputation, 0.05, \"alcohol_5_percent\")\n",
    "create_missing(df_imputation, 0.1, \"alcohol_10_percent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9APk94LA4ko"
   },
   "outputs": [],
   "source": [
    "# Performing Mean Imputation\n",
    "mm_imputed_df_imputation = mean_impute(df_imputation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9WKEM2pBB94"
   },
   "source": [
    "For MICE imputation, we have to be carefull and create three different dataframes for all three configuration, because MICE imputation uses all features in the data to predict(impute) the missing value. Having all coloumns in one dataframe can lead to the issue of co-linearity which will mislead the regression model and result in bad imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqT-PBaXA8FC"
   },
   "outputs": [],
   "source": [
    "# Dataframe for 1% missing value\n",
    "df_imputation_1_percent = df_imputation.drop(\n",
    "    [\"alcohol\", \"alcohol_5_percent\", \"alcohol_10_percent\"], axis=1\n",
    ")\n",
    "# Dataframe for 5% missing value\n",
    "df_imputation_5_percent = df_imputation.drop(\n",
    "    [\"alcohol\", \"alcohol_1_percent\", \"alcohol_10_percent\"], axis=1\n",
    ")\n",
    "# Dataframe for 10% missing value\n",
    "df_imputation_10_percent = df_imputation.drop(\n",
    "    [\"alcohol\", \"alcohol_1_percent\", \"alcohol_5_percent\"], axis=1\n",
    ")\n",
    "\n",
    "# Performing Imputation on Dataframe with 1% missing values\n",
    "mice_imputed_df_imputation_1_percent = mice_impute(df_imputation_1_percent)\n",
    "# Performing Imputation on Dataframe with 5% missing values\n",
    "mice_imputed_df_imputation_5_percent = mice_impute(df_imputation_5_percent)\n",
    "# Performing Imputation on Dataframe with 10% missing values\n",
    "mice_imputed_df_imputation_10_percent = mice_impute(df_imputation_10_percent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmZKVf5iBGwp"
   },
   "source": [
    "Lets see how our imputation methods have recovered the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2P_BgPHA9tt",
    "outputId": "9642b232-2fe5-4de7-a725-46ba399da145"
   },
   "outputs": [],
   "source": [
    "# This is the average % error on residuals for 1% missing data imputed using Mean Imputation\n",
    "print(\"Average % error on residuals for 1% missing data imputed using Mean Imputation\")\n",
    "print(\n",
    "    percentage_change(\n",
    "        mm_imputed_df_imputation[\"alcohol\"],\n",
    "        mm_imputed_df_imputation[\"alcohol_1_percent\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# This is the average % error on residuals for 1% missing data imputed using MICE Imputation\n",
    "print(\"Average % error on residuals for 1% missing data imputed using MICE Imputation\")\n",
    "print(\n",
    "    percentage_change(\n",
    "        df_imputation[\"alcohol\"],\n",
    "        mice_imputed_df_imputation_1_percent[\"alcohol_1_percent\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# This is the average % error on residuals for 5% missing data imputed using Mean Imputation\n",
    "print(\"Average % error on residuals for 5% missing data imputed using Mean Imputation\")\n",
    "print(\n",
    "    percentage_change(\n",
    "        mm_imputed_df_imputation[\"alcohol\"],\n",
    "        mm_imputed_df_imputation[\"alcohol_5_percent\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# This is the average % error on residuals for 5% missing data imputed using MICE Imputation\n",
    "print(\"Average % error on residuals for 5% missing data imputed using MICE Imputation\")\n",
    "print(\n",
    "    percentage_change(\n",
    "        df_imputation[\"alcohol\"],\n",
    "        mice_imputed_df_imputation_5_percent[\"alcohol_5_percent\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# This is the average % error on residuals for 10% missing data imputed using Mean Imputation\n",
    "print(\"Average % error on residuals for 10% missing data imputed using Mean Imputation\")\n",
    "print(\n",
    "    percentage_change(\n",
    "        mm_imputed_df_imputation[\"alcohol\"],\n",
    "        mm_imputed_df_imputation[\"alcohol_10_percent\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# This is the average % error on residuals for 10% missing data imputed using MICE Imputation\n",
    "print(\"Average % error on residuals for 10% missing data imputed using MICE Imputation\")\n",
    "print(\n",
    "    percentage_change(\n",
    "        df_imputation[\"alcohol\"],\n",
    "        mice_imputed_df_imputation_10_percent[\"alcohol_10_percent\"],\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQQBCRsrBf9s"
   },
   "source": [
    "# Interpreting the Imputation Methods\n",
    "- From the above 6 readings we can conclude that the MICE imputation works well when compared to the Mean imputation method as the imputed value is a function of rest of the features. \n",
    "- We can also observe that the % error of residuals increase as the missing data points increase. From this we can conclude that if we have less missing data in the dataset, more perfectly it is imputed back by the imputation method. \n",
    "\n",
    "# Why Imputing data is always better than deleting the row ?\n",
    "- For obvious reason - Data Loss. When we delete a row we end up deleting a sample from training data which will do more harm to the model instead of doing any good.\n",
    "- Selecting of imputation method is really a personal choice, although from the above analysis we can come to the conclusion that MICE does a better job in getting the databack, but after creating a model it hardly has any positive effects on the accuracy of model. But it is always a better choice to use MICE as it can give you the closest possible value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maRqG3fZBm-g"
   },
   "source": [
    "# Feature Importance and Selection\n",
    "- Lets fit a very simple linear model to understand how the features of wine are affecting its quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nc8vcoSIBVbC",
    "outputId": "f804bfd3-958f-4e78-87b4-fe9e0a6541f2"
   },
   "outputs": [],
   "source": [
    "# Scaling the data using Min-Max scaling for the purpose of evaluating Co-efficients, as non scaled data may mislead while we evaluate the co-efficients\n",
    "df = wine_df_pandas.drop([\"quality\"], axis=1)\n",
    "df_norm = (df - df.min()) / (df.max() - df.min())\n",
    "df_norm = pd.concat((df_norm, wine_df_pandas.quality), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdGH6TOfCG3T",
    "outputId": "ec9dd832-d138-4ac9-d00e-084128b90390"
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf  # OLS model Library\n",
    "\n",
    "results = smf.ols(\n",
    "    \"quality ~ fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide +density + pH + sulphates + alcohol\",\n",
    "    data=df_norm,\n",
    ").fit()\n",
    "print(results.summary())  # OLS Linear Model Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LV6r2ioGCPTA"
   },
   "source": [
    "# Feature Selection\n",
    "Feature Selection is the process of selecting the features which are relevant to a machine learning model. It means that you select only those attributes that have a significant effect on the modelâ€™s output.\n",
    "\n",
    "Consider the case when you go to the departmental store to buy grocery items. A product has a lot of information on it, i.e., product, category, expiry date, retail cost, ingredients, and manufacturing details. All this information is the features of the product. Normally, you check the brand, retail cost, expiry date, weather the item is vegetarian or not before buying a product. However, manufacturing section is not your concern. Therefore, brand, retail cost, expiry date, vegetraian/non-vegetarian are relevant features, and the manufacturing details are irrelevant. This is how feature selection is done. \n",
    "\n",
    "# Null-Hypothesis\n",
    "### For Example:- Testing the efficacy of a new Vaccine\n",
    "Any new vaccine manufacturer has to conduct a double blind clinical controlled trials to check the efficacy of any vaccine in the general population. Now to test the vaccine, manufacturer will likely recruite around 60k people from all over the population where 30k would be vaccinated by a vaccine and rest 30k would be given a placebo. After few months the it was found that 500 people from vaccinated group got the diesease whereas 10000 people from placebo group got the dieases. This translate to a vaccine efficacy of 95%. i.e. those who received the vaccine were at a 95% lower risk of developing disease than the group who received the placebo. Now, this finding on a subset of 60k individuals is generalized over the whole population of man kind. These assumption is known as Null-Hypothesis.  \n",
    "\n",
    "Statistically, this can be achieved by P-Value.\n",
    "\n",
    "**P-Value** - It stands for â€˜probability valueâ€™; it tells how likely it is that a result occurred by chance alone. Basically, the p-value is used in hypothesis testing to help you support or reject the null hypothesis. The smaller the p-value, the stronger the evidence to reject the null hypothesis.\n",
    "\n",
    "Features in our dataset with P value < 0.05\n",
    "*   volatile_acidity\n",
    "*   chlorides\n",
    "*   total_sulfur_dioxide\n",
    "*   sulphates\n",
    "*   alcohol\n",
    "\n",
    "Lets consider only these features and calculate the model fit again, and then lets have a comparative analysis of both models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sY20vGx8CKFF"
   },
   "outputs": [],
   "source": [
    "df_norm_feature_selected = df_norm[\n",
    "    [\n",
    "        \"volatile_acidity\",\n",
    "        \"chlorides\",\n",
    "        \"total_sulfur_dioxide\",\n",
    "        \"sulphates\",\n",
    "        \"alcohol\",\n",
    "        \"pH\",\n",
    "        \"quality\",\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAnplZryCRwK",
    "outputId": "a6bf021b-19bf-4814-aecd-22d94be0c79d"
   },
   "outputs": [],
   "source": [
    "results2 = smf.ols(\n",
    "    \"quality ~ volatile_acidity + chlorides + total_sulfur_dioxide + sulphates + alcohol + pH\",\n",
    "    data=df_norm_feature_selected,\n",
    ").fit()\n",
    "print(results2.summary())  # OLS Linear Model Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqbW0Nw7DdJY"
   },
   "outputs": [],
   "source": [
    "predicted_quality_1 = results.predict(\n",
    "    df_norm[\n",
    "        [\n",
    "            \"fixed_acidity\",\n",
    "            \"volatile_acidity\",\n",
    "            \"citric_acid\",\n",
    "            \"residual_sugar\",\n",
    "            \"chlorides\",\n",
    "            \"free_sulfur_dioxide\",\n",
    "            \"total_sulfur_dioxide\",\n",
    "            \"density\",\n",
    "            \"pH\",\n",
    "            \"sulphates\",\n",
    "            \"alcohol\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "df_norm[\"predicted_quality_1\"] = predicted_quality_1.round()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhWwoscyEWqd",
    "outputId": "f4896e2b-2c24-4412-fd54-cb10f11a5bbc"
   },
   "outputs": [],
   "source": [
    "predicted_quality_2 = results2.predict(\n",
    "    df_norm_feature_selected[\n",
    "        [\n",
    "            \"volatile_acidity\",\n",
    "            \"chlorides\",\n",
    "            \"total_sulfur_dioxide\",\n",
    "            \"sulphates\",\n",
    "            \"alcohol\",\n",
    "            \"pH\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "df_norm_feature_selected[\"predicted_quality_2\"] = predicted_quality_2.round()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0Kx19KmFi5-",
    "outputId": "b7966c49-5885-44d8-e0f3-30f7f4a54763"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Accuracy of predicting the correct quality of wine using all features = \",\n",
    "    100 - percentage_change(df_norm[\"quality\"], df_norm[\"predicted_quality_1\"]),\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of predicting the correct quality of wine using only significant features is = \",\n",
    "    100\n",
    "    - percentage_change(\n",
    "        df_norm_feature_selected[\"quality\"],\n",
    "        df_norm_feature_selected[\"predicted_quality_2\"],\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8gFD310CXzR"
   },
   "source": [
    "- From the above two summaries we can interpret that after selecting only the significant features(features with P value less than 0.05) it had little to no effect on the outcome compare to the model which uses all the features.\n",
    "\n",
    "- From the above two interpretations, our observation is that in both the cases our model predicts the quality of wine pretty good and they are pretty close to each other. Model who uses all features slighly overcomes the model who is using only the statistically important features with and accuracy of **92.432% compared to the 92.43 %** of the later\n",
    "\n",
    "- From the above studies let's stick to the data with all featues and try using different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-XtN_YCKCIG"
   },
   "outputs": [],
   "source": [
    "df_norm = df_norm.drop([\"predicted_quality_1\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmZGV7kIKU_Z"
   },
   "outputs": [],
   "source": [
    "X = df_norm.drop([\"quality\"], axis=1)  # Training Feature\n",
    "Y = df_norm[\"quality\"]  # Target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsspPq956WuE"
   },
   "source": [
    "### How did you split the data into train, and test?\n",
    "\n",
    "- The data was splitted into Training ad Testing Data into 90% and 10% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HxHk2itLMO_"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    ") = train_test_split(  # Splitting Data into Training and Test\n",
    "    X, Y, test_size=0.1, random_state=42\n",
    ")\n",
    "x_train_100 = shap.utils.sample(\n",
    "    x_train, 100\n",
    ")  # Taking 100 samples out for SHAP analysis as it is a computationally expensive process\n",
    "x_test_100 = shap.utils.sample(\n",
    "    x_test, 100\n",
    ")  # Taking 100 samples out for SHAP analysis as it is a computationally expensive process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYWlvsyCMO98"
   },
   "source": [
    "### Fitting a Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSqKhmd8L8CZ",
    "outputId": "c750417e-52f4-44c7-f44d-7ec64c5916e6"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "linear_model = sklearn.linear_model.LinearRegression()  # Initializing a Linear Model\n",
    "linear_model.fit(x_train, y_train)  # Training a linear model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEgBMwFKMS4h"
   },
   "outputs": [],
   "source": [
    "y_linear_predictions = linear_model.predict(x_test).round()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ugo70KwM783"
   },
   "source": [
    "### Fitting a Tree Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pG9F2FPKMjyI",
    "outputId": "aa30c37d-1651-4a96-f00f-c56893a90344"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "tree_model = RandomForestRegressor(\n",
    "    max_depth=X.shape[1], random_state=0, n_estimators=10\n",
    ")\n",
    "tree_model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuukpWOsM0lB"
   },
   "outputs": [],
   "source": [
    "y_tree_based_predictions = tree_model.predict(x_test).round()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY82S5_kOBur"
   },
   "source": [
    "### Fitting a Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKFKCcqENHgM"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "regr = svm.SVR()\n",
    "svm_model = regr.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KbhKcRLNlBj"
   },
   "outputs": [],
   "source": [
    "svm_predictions = svm_model.predict(x_test).round()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjNgXLhAT3Kr"
   },
   "source": [
    "### Fitting a MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-wm-1SgSVQD"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "regr = MLPRegressor(random_state=1, max_iter=500).fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3SBcHT3SbQA"
   },
   "outputs": [],
   "source": [
    "mlp_predictions = regr.predict(x_test).round()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDPzKtNv612f"
   },
   "source": [
    "### Using AutoML to find out the best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Gx-8_GiQlZM"
   },
   "outputs": [],
   "source": [
    "# Setting up maximum runtime for the AutoML\n",
    "min_mem_size = 6\n",
    "run_time = 222\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4r4cpXCQwaY",
    "outputId": "41dad4ee-185a-4e64-fc8a-7eaf4e9168ea"
   },
   "outputs": [],
   "source": [
    "# Setting up memory\n",
    "pct_memory = 0.5\n",
    "virtual_memory = psutil.virtual_memory()\n",
    "min_mem_size = int(round(int(pct_memory * virtual_memory.available) / 1073741824, 0))\n",
    "print(min_mem_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "NtzwtsS9UC2W",
    "outputId": "587eeea1-8e1b-4ab2-975a-a3030be336ec"
   },
   "outputs": [],
   "source": [
    "# 65535 Highest port no\n",
    "# Start the H2O server on a random port\n",
    "port_no = random.randint(5555, 55555)\n",
    "\n",
    "#  h2o.init(strict_version_check=False,min_mem_size_GB=min_mem_size,port=port_no) # start h2o\n",
    "try:\n",
    "    h2o.init(\n",
    "        strict_version_check=False, min_mem_size_GB=min_mem_size, port=port_no\n",
    "    )  # start h2o\n",
    "except:\n",
    "    logging.critical(\"h2o.init\")\n",
    "    h2o.download_all_logs(dirname=logs_path, filename=logfile)\n",
    "    h2o.cluster().shutdown()\n",
    "    sys.exit(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQST2_DcUEda",
    "outputId": "cb4d192e-f6ad-4cf0-f4fa-6604c7282bb4"
   },
   "outputs": [],
   "source": [
    "wine_hf = h2o.H2OFrame(wine_df_pandas)  # Converting Pandas dataframe in H2O dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uM_6GL87XCPE"
   },
   "outputs": [],
   "source": [
    "pct_rows = 0.80  # Setting up the data split between training and test dataset.\n",
    "df_train, df_test = wine_hf.split_frame([pct_rows])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7iLZQ7uUTmk"
   },
   "outputs": [],
   "source": [
    "# Setting up AutoML\n",
    "def autoML(df_train, df_test):\n",
    "\n",
    "    X = df_train.columns\n",
    "    y = \"quality\"\n",
    "\n",
    "    X.remove(y)  # Removing the result frm our predictors data\n",
    "\n",
    "    aml = H2OAutoML(max_runtime_secs=222, seed=1)  # Setting of AutoML\n",
    "    aml.train(\n",
    "        x=X, y=y, training_frame=df_train\n",
    "    )  # Trainig the dataset on different models\n",
    "\n",
    "    return df_train, df_test, aml\n",
    "\n",
    "\n",
    "# Function to get best model from the AutoML leaderboard\n",
    "def getBestModel(aml):\n",
    "    model_index = 0\n",
    "    glm_index = 0\n",
    "    aml_leaderboard_df = aml.leaderboard.as_data_frame()\n",
    "    models_dict = {}\n",
    "\n",
    "    for m in aml_leaderboard_df[\"model_id\"]:\n",
    "        models_dict[m] = model_index\n",
    "        if \"StackedEnsemble\" not in m and \"GBM\" not in m:\n",
    "            break\n",
    "        model_index = model_index + 1\n",
    "\n",
    "    for m in aml_leaderboard_df[\"model_id\"]:\n",
    "        if \"GLM\" in m:\n",
    "            models_dict[m] = glm_index\n",
    "            break\n",
    "        glm_index = glm_index + 1\n",
    "\n",
    "    print(model_index)\n",
    "    best_model = h2o.get_model(aml.leaderboard[model_index, \"model_id\"])\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95I4PM8XUiLJ",
    "outputId": "acd19e07-4fd3-49ef-a368-e2c9065e1ed5"
   },
   "outputs": [],
   "source": [
    "autoML = autoML(df_train, df_test)  # Training AutoML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "6Q5Ic8aSUmQx",
    "outputId": "61583360-0310-48b2-c815-14cfc209a9ff"
   },
   "outputs": [],
   "source": [
    "autoML[2].leaderboard  # Leaderbord of AutoML output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bteJYEoHU2us",
    "outputId": "4198ebde-4c87-4add-a7a4-ad2707a47a59"
   },
   "outputs": [],
   "source": [
    "autoML_model = getBestModel(autoML[2])  # Getting Best Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2h0bWldQVn4v",
    "outputId": "3e6ec65e-e9f4-4a51-c809-913a269b9536"
   },
   "outputs": [],
   "source": [
    "autoML_best_predictions = autoML_model.predict(df_test).round()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uLhWMQAZXCP"
   },
   "outputs": [],
   "source": [
    "autoMLpredictions = h2o.as_list(autoML_best_predictions, use_pandas=False)\n",
    "y_test_h2O = h2o.as_list(df_test[\"quality\"], use_pandas=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naPm6Gs-d8Pf"
   },
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    listt = [(item) for sublist in t for item in sublist][1:]\n",
    "    listt = [float(x) for x in listt]\n",
    "    x = pd.Series(listt)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6vUtAscecLF"
   },
   "outputs": [],
   "source": [
    "autoMLpredictions = flatten(autoMLpredictions)\n",
    "y_test_h2O = flatten(y_test_h2O)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3hjmaqisjof"
   },
   "source": [
    "# Hyperparameter Tuning of AutoML's Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Un0eu1fXsn-3",
    "outputId": "d06af1f2-135e-4e39-b629-17c323d2fda6"
   },
   "outputs": [],
   "source": [
    "train, validate, test = np.split(\n",
    "    wine_df_pandas.sample(frac=1, random_state=42),\n",
    "    [int(0.6 * len(df)), int(0.8 * len(df))],\n",
    ")\n",
    "train = h2o.H2OFrame(train)\n",
    "validate = h2o.H2OFrame(validate)\n",
    "test = h2o.H2OFrame(test)\n",
    "\n",
    "drf_hyper_params = {\n",
    "    \"ntrees\": [10, 25, 50, 75, 100],\n",
    "    \"max_depth\": [5, 7, 10, 14],\n",
    "    \"sample_rate\": [0.25, 0.5, 0.75, 1.0],\n",
    "}\n",
    "\n",
    "search_criteria = {\"strategy\": \"RandomDiscrete\", \"max_models\": 100, \"seed\": 1}\n",
    "\n",
    "gbm_grid2 = H2OGridSearch(\n",
    "    model=H2ORandomForestEstimator,\n",
    "    grid_id=\"gbm_grid2\",\n",
    "    hyper_params=drf_hyper_params,\n",
    "    search_criteria=search_criteria,\n",
    ")\n",
    "\n",
    "gbm_grid2.train(\n",
    "    x=list(X.columns),\n",
    "    y=\"quality\",\n",
    "    training_frame=train,\n",
    "    validation_frame=validate,\n",
    "    seed=1,\n",
    ")\n",
    "\n",
    "\n",
    "def find_best_model_from_grid(h2o_grid, test_parameter):\n",
    "    model_list = []\n",
    "    for grid_item in h2o_grid:\n",
    "        if test_parameter is \"r2\":\n",
    "            if not (grid_item.r2() == \"NaN\"):\n",
    "                model_list.append(grid_item.r2())\n",
    "            else:\n",
    "                model_list.append(0.0)\n",
    "        elif test_parameter is \"auc\":\n",
    "            if not (grid_item.auc() == \"NaN\"):\n",
    "                model_list.append(grid_item.auc())\n",
    "            else:\n",
    "                model_list.append(0.0)\n",
    "    max_index = model_list.index(max(model_list))\n",
    "    best_model = h2o_grid[max_index]\n",
    "    print(\"Model ID with best R2: \" + best_model.model_id)\n",
    "    if test_parameter is \"r2\":\n",
    "        print(\"Best R2: \" + str(best_model.r2()))\n",
    "    elif test_parameter is \"auc\":\n",
    "        print(\"Best AUC: \" + str(best_model.auc()))\n",
    "    return best_model\n",
    "\n",
    "\n",
    "best_drf_model = find_best_model_from_grid(gbm_grid2, \"r2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpidOTPCKTul"
   },
   "source": [
    "### What evaluation metric are you using?\n",
    "\n",
    "- The evaluation metrics which I am using are \n",
    "    - Mean Squared Error\n",
    "    - Root Mean Squared Error\n",
    "    - Mean Absolute Error\n",
    "    - Mean Residual Deviance\n",
    "    - Accuracy\n",
    "- These evaluation metrics are used to evaluate the best model, as it is a regression problem. Here, Mean Residual Deviance and Accuracy are the most important metrics used to evaluate the mode. \n",
    "\n",
    "- The residual deviance tells us how well the response variable can be predicted by a model with p predictor variables. The lower the value, the better the model is able to predict the value of the response variable.\n",
    "\n",
    "- Accuracy tells us how correct and precise our model is. \n",
    "\n",
    "### How do training, validation, and test metrics compare?\n",
    "\n",
    "- The best model (AutoML's Hyperparameter tuned model) has done pretty well on both the Training Dataset as well as Validation Dataset. \n",
    "\n",
    "- As the Mean Residual Deviance of this model on training and validation dataset is ~0.3929 and ~0.3534 respectively. Also, the other metrics on both of the dataset validates that the model is not overfitting the Training Data. Please refer below to see the Evaluation Metrics on both training and validation dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eq21PDrvJrzr",
    "outputId": "5d308967-71ab-410b-8cc1-b3d2276b8796"
   },
   "outputs": [],
   "source": [
    "best_drf_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_o7aYt5tV_r",
    "outputId": "56d958b4-bbda-4c3c-99b5-77f07606251b"
   },
   "outputs": [],
   "source": [
    "# Evaluating the model on Test Data\n",
    "autoML_best_predictions_2 = best_drf_model.predict(df_test).round()\n",
    "autoMLpredictions_2 = h2o.as_list(autoML_best_predictions_2, use_pandas=False)\n",
    "y_test_h2O_2 = h2o.as_list(df_test[\"quality\"], use_pandas=False)\n",
    "autoMLpredictions_2 = flatten(autoMLpredictions_2)\n",
    "y_test_h2O_2 = flatten(y_test_h2O_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5vLiPtItx9a"
   },
   "source": [
    "### Which models did you explore and did you try to tune the hyperparameters of the best model you got?\n",
    "- I trained multiple models for this purpose including the simplest Linear Regression to using AutoML to get the best model it could fit. The models I trained for this purpose were \n",
    "    - Linear Regressor\n",
    "    - Random Forest Regressor\n",
    "    - MLP Regressor\n",
    "    - Support Vector Machine(SVM) Regressor\n",
    "    - AutoML\n",
    "- The models which gave me the best performance in predicting the quality of wine were the SVM and the model from AutoML where the model from AutoML slightly outpaced the SVM. The model from AutoML to give the best result was a RandomForestRegressor.\n",
    "\n",
    "- As the AutoML's model was promising, I tuned its hyperparameters to oprimize the model. Tunining the hyperparameters helped the model to incease its accuracy from **93.25 % to 96.08 %** which is a great increase in the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jjGlB3BWtxF8",
    "outputId": "3a00abba-08e2-4b93-a502-1ee09e0c6ee7"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Accuracy of predicting the correct quality of wine using MLP Regressor = \",\n",
    "    100 - percentage_change(y_test, mlp_predictions),\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of predicting the correct quality of wine using Linear Model = \",\n",
    "    100 - percentage_change(y_test, y_linear_predictions),\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of predicting the correct quality of wine using Tree Based Model = \",\n",
    "    100 - percentage_change(y_test, y_tree_based_predictions),\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of predicting the correct quality of wine using SVM = \",\n",
    "    100 - percentage_change(y_test, svm_predictions),\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of predicting the correct quality of wine using best AutoML model = \",\n",
    "    100 - percentage_change(y_test_h2O, autoMLpredictions),\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of predicting the correct quality of wine using best AutoML model with Tuned Hyper-Parameters = \",\n",
    "    100 - percentage_change(y_test_h2O_2, autoMLpredictions_2),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aws1DdRsUg8m"
   },
   "source": [
    "# Model Selection\n",
    "From the above summary of the models trained on the dataset, the two best models are SVM and AutoML's best model with tuned hyperparameters. \n",
    "*   SVM Regressor with an Accuracy of ~ 93.37%\n",
    "*   AutoML's Hyperparameter Tuned Version ~ 96.20%\n",
    "\n",
    "Let's try to understand how both the models have been trained.\n",
    "\n",
    "**Interpreting SVM Regressor using SHAP values**\n",
    "- Here the x-axis is the feature and the y-axis is the output as we vary the feature. The grey histogram is the distribution of variables in the dataset and the cross made by E[Feature], E[f(x)] is the expected values.\n",
    "\n",
    "- Let us take a feature of fixed_acidity\n",
    "  - The cross is made at approx  E[f(x)] 5.74\n",
    "\n",
    "So as the fixed_acidity increases the expected value also increases\n",
    "\n",
    "Talking about the red line on the plot - When we give a sample as an input (sample_ind = 18) as an input to check the output. By plotting this we can see the difference between the model output from the expected value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNrE7P20fB28",
    "outputId": "5c13bb70-9003-46da-962a-e30d94d990e4"
   },
   "outputs": [],
   "source": [
    "svm_explainer = shap.Explainer(svm_model.predict, x_train_100)\n",
    "svm_shap_values = svm_explainer(x_train_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TaOEo0Onfrsr",
    "outputId": "1a54ff52-b2fd-4b99-b77c-76300931a7cb"
   },
   "outputs": [],
   "source": [
    "for i in x_train_100.columns:\n",
    "    # make a standard partial dependence plot\n",
    "    sample_ind = 18\n",
    "    shap.partial_dependence_plot(\n",
    "        i,\n",
    "        svm_model.predict,\n",
    "        x_train_100,\n",
    "        model_expected_value=True,\n",
    "        feature_expected_value=True,\n",
    "        ice=False,\n",
    "        shap_values=svm_shap_values[sample_ind : sample_ind + 1, :],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJo3Jo3tQBRg"
   },
   "source": [
    "**Interpreting SHAP Feature Importance Plot for Linear and Tree-based model**\n",
    "- The idea behind SHAP feature importance is simple: Features with large absolute Shapley values are important. Since we want global importance, we average the absolute Shapley values per feature across the data. Next, we sort the features by decreasing importance and plot them.\n",
    "\n",
    "- The following plot is \n",
    "  - SHAP feature importance plot for SVM model\n",
    "\n",
    "- Using the below plot we can come on the conclusion that the alcohol is the most important feature, followed by volatile_acidity and sulphate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "uVesAizXlQnC",
    "outputId": "1c95a3b6-0bfc-4989-b44f-bf85dfee429a"
   },
   "outputs": [],
   "source": [
    "# SHAP variable importance for a Linear Model\n",
    "shap.summary_plot(svm_shap_values, x_train, plot_type=\"bar\", color=\"blue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nATyiQ3-liRW"
   },
   "outputs": [],
   "source": [
    "# Waterfall plot for linear model\n",
    "def get_SHAP(index=18):\n",
    "    shap.plots.waterfall(svm_shap_values[index], max_display=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a2DIAn8Qr_o"
   },
   "source": [
    "#Interpreting Waterfall SHAP visualization\n",
    "\n",
    "- Let's consider the same sample (sample_ind = 18).\n",
    "It says that **f(x) = 5.9 is what we got as a model output** and the **expected output for this sample was 5.725**. We came pretty close to determining it as the **difference is only 0.175**. The waterfall model explains how we got the expected output, and which features contributed to what. The below graph shows that volatile_acidity has the biggest and most positive impact in **increasing the quality of wine by 0.12** for this specific sample. Followed by alcohol had a negative impact and it bought the **quality of wine down again by 0.06** for this sample, and so on. Using this model we can visually interpret why exactly this specific sample is giving an output of 5.725\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "U9vzeo8mltZb",
    "outputId": "e01909a0-dd52-4a5c-e94d-29af4f225afc"
   },
   "outputs": [],
   "source": [
    "get_SHAP()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5tZMc-nRYxW"
   },
   "source": [
    "#Interpret the summary plot SVM Model\n",
    "- Here the features are listed in descending order of their importance. This is one of the easiest ways to analyze an ML model and how the features are affecting the target and to what extent. \n",
    "\n",
    "  1. Each dot(both red and blue) represents a feature of a wine\n",
    "  2. Red color represents high values whereas blue color represents low value\n",
    "  3. If a dot(a feature of wine) is on the right side of the y-axis then it had a positive impact and if it is on the left side of the axis it had a negative impact\n",
    "  4. The position of a dot(a feature of wine) on the x-axis represents the intensity of impact it had, the more it is away from the axis greater the intensity.\n",
    "\n",
    "- Let us try to understand how the features are affecting the model.\n",
    "\n",
    "  *   Higher value of alcohol tends to have a positive impact on the quality of wine\n",
    "  *   Lower values of volatile_acidity tend to have a positive impact on the quality of the wine.\n",
    "  *  Higher values of sulfates tends to have a positive impact on the quality of wine\n",
    "  * Lower value of total_sulphur_dioxide tends to have a positive impact on the quality of wine, and so on. \n",
    "\n",
    "- We could also interpret the intensity of impact\n",
    "  - Higher amount of sulfates have a much positive impact on the quality of the wine but it doesn't really care if the value is low it will always have the same intensity of negative impact on the quality\n",
    "\n",
    "- This visualization is very useful when it comes to interpreting how our model is working.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "LnkO63e9l-A9",
    "outputId": "072c45e6-b250-4e4f-be37-9477856b7207"
   },
   "outputs": [],
   "source": [
    "# SHAP summary for Linear Model\n",
    "shap.summary_plot(svm_shap_values, x_train_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "u8STCibrmEjH",
    "outputId": "2b561e48-fce7-4550-d910-db0e0c223086"
   },
   "outputs": [],
   "source": [
    "# SHAP partial dependence plot for a tree based model\n",
    "for i in x_train.columns:\n",
    "    shap.dependence_plot(i, svm_shap_values.values, x_train_100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pt21MaHBSBjv"
   },
   "source": [
    "###Heatmap Visualization for Linear and Tree-based model\n",
    "\n",
    "- This visualization is a summarization of the entire dataset on how each data point in every feature is affecting the target(quality of wine). \n",
    "  - Here the Y-axis is Features\n",
    "  - and X-axis is Instance of that feature\n",
    "  - The color of the instance defines if it had a positive effect or negative effect by its color. If the instance is red then it had a positive effect and if the instance is blue then it had a negative effect\n",
    "  - The intensity of the color is directly proportional to the intensity of the effect. Deeper the color, the more impactful the feature is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "NHN_FFaemM-X",
    "outputId": "667e2d1a-4413-4f2c-b70c-a6cca9f5a174"
   },
   "outputs": [],
   "source": [
    "shap.plots.heatmap(svm_shap_values)  # SHAP HeatMap of a Tree Based Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3Ul2DDopmY3Y",
    "outputId": "633e105a-66b7-45d0-98a4-db3d018c41ff"
   },
   "outputs": [],
   "source": [
    "# PDP Plot for Tree based Model\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "plot_partial_dependence(svm_model, x_test, x_test.columns, n_jobs=3, grid_resolution=20)\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "fig.suptitle(\"Partial dependence of Wine Quality Features\")\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.8)\n",
    "plt.rcParams[\"figure.figsize\"] = (32, 24)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGSmtmJwlVP3"
   },
   "source": [
    "# Interpreting AutoML's best model with tuned hyperparameters\n",
    "### How are errors/residuals distributed and how interpretable is your model ?\n",
    "**Residual Analysis**\n",
    "- Here, we can see the striped lines of residuals, which is an artifact of having an integer value as a response value, instead of a real value. It can also be observed from the below graph that residuals are normally distributed. Residuals signify that residuals don't have heteroscedasticity.\n",
    "\n",
    "**Variable Importance**\n",
    "- In the variable importance plot we can observe that the plot replicates the results which we got from the Linear Model and Tree-Based Model above. The variable importance has been scaled between 0 to 1 for ease of understanding\n",
    "\n",
    "**SHAP Summary**\n",
    "- From the SHAP summary diagram, we can interpret a few conclusions - \n",
    "\n",
    "  1.   All the features are listed as per their importance in making the prediction, that is alcohol is more significant followed by volatile_acidity, and so on. Hence, free_sulphur_dioxide is the least significant predictor in our model.\n",
    "  2.   The position on the SHAP value axis indicates the impact the feature has on the prediction either positive or negative. That is, the more the data point is away from the 0.0 shap value - the more its impact is. As we can see Alcohol has the most impact on the quality of the wine. The color of the dot represents (Red - High, Blue - Low) the impact of the value on the result. i.e. - Hight amount of alcohol results in high quality of wine, a low amount of volatile acidity results in high quality of wine, and so on. As we can see the distribution of red points in the Alcohol feature is far more spread than the blue points. From this visual, we can interpret that the quality of wine vastly increases as the amount of alcohol increases. It doesn't have much impact if the content is low i.e. between the shap value of 0.4 to 0.6 the blue data points are pretty concentrated. Inversely for the Volatile Acidity, less is better for the quality of the wine. Let us analyze the distribution of SHAP values of this feature. As seen in the SHAP plot we can see that extreme values on both ends can have a significant effect on the quality of the wine. \n",
    "\n",
    "\n",
    "- The advantage of SHAP analysis over normal feature importance is that we could visualize how the feature is affecting the target at different values. The standard methods tend to overestimate the importance of continuous or high-cardinality categorical variables.\n",
    "\n",
    "**Partial Dependence Plot (PDP)**\n",
    "\n",
    "- A partial dependence plot shows the marginal effect of a feature on the target(wine quality in our dataset). It is achieved by keeping all other variables constant and changing the value of one variable to get its PDP. For Interpretation purposes, let us pick up the two most important variables - Alcohol and Sulphate. \n",
    "\n",
    "  1.   As we can see that when the rest of the variables are kept constant and a marginal change is made in alcohol, we can see the mean response increases between alcohol levels of 11 and 12. This could be interpreted as this range of alcohol could be the deciding factor in the quality of the wine.\n",
    "  2.   Similarly when the rest of the variables are kept constant and a marginal change is made in sulfate we can observe the mean response of quality going up between the sulfate range of 0.4 to 0.8. Hence, it can be interpreted that this range is deciding factor in the quality of the wine. \n",
    "\n",
    "- The computation of partial dependence plots is intuitive: The partial dependence function at a particular feature value represents the average prediction if we force all data points to assume that feature value.\n",
    "\n",
    "**ICE (Individual Conditional Expectation) Plot**\n",
    "- ICE plot is similar to what we did in the PDP plot, but the fundamental difference between the two methods is that the PDP plot focuses on the average effect of a feature and does not focus on a specific instance. ICE plot comes in rescue to address this drawback of the PDP plot where the outputs from all instances are considered instead of an average value. \n",
    "\n",
    "- Let us go back to the alcohol and sulfate features, as we interpreted in PDP the quality of wine increases when alcohol and sulfate value is between 11 to 12 and 0.4 to 0.8 respectively. But is this true for every wine in the dataset? ICE plot has an answer to this question. As we can see in the ICE plot above the quality of a few wines(0th percentile instance) increases a lot in this range while the quality of a few doesn't change much(100th Percentile Instance). \n",
    "\n",
    "- Individual conditional expectation curves are even more intuitive to understand than partial dependence plots. One line represents the predictions for one instance if we vary the feature of interest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_2xF5F2BZL8D",
    "outputId": "2e79a90d-2e4f-4020-c6a3-5d9f76a83e28"
   },
   "outputs": [],
   "source": [
    "best_drf_model.explain(autoML[1])  # AutoML Explainability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Iy6wWMdWJPM"
   },
   "source": [
    "### Final Takeaways\n",
    "- The AutoML's best model with its Hyperparameter's tuned is the best model for this task as it has the highest accuracy compared to the other models tested in this excercise. Also, this model is interpretible i.e. we can actually get to the root of what made this prediction to come to. \n",
    "\n",
    "### Learning Outcomes\n",
    "- I learned the complete lifecycle of a Data Science project right from data prepartion to hyperparameter tuning\n",
    "- Majority of the time should be invested in data preparation i.e. cleaning the data, normalizing, feature selection, imputation etc\n",
    "- Hyperparameter tuning is the second most important thing after data preparation, which most of the practioner's ignore. But the results are worth the time invested\n",
    "- Multiple models must be trained and the best models should be selected to be deployed, as some algorithms perform much better than the other's on specific tasks\n",
    "- Model Interpretation(Unboxing the Black Blox) is the best takeaway from the series of this assignments. SHAP, LIME and PDP have made it easier to understand what made a model to predict a outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-QA73CQDTUm"
   },
   "source": [
    "# References\n",
    "- https://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html\n",
    "- https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xQQBCRsrBf9s"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
